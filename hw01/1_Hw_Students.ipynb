{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "VFj8-qGfYA-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "cSuFlZPnrT8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/deep_vision_and_graphics/fall22/week01-pytorch_intro/notmnist.py\n",
        "    !touch .setup_complete"
      ],
      "metadata": {
        "id": "usRNEECdbR9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1. Tensors (1 point)"
      ],
      "metadata": {
        "id": "u7FaNwW2X_v0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write another function, this time in polar coordinates:\n",
        "$$\\rho(\\theta) = (1 + 0.9 \\cdot cos (8 \\cdot \\theta) ) \\cdot (1 + 0.1 \\cdot cos(24 \\cdot \\theta)) \\cdot (0.9 + 0.05 \\cdot cos(200 \\cdot \\theta)) \\cdot (1 + sin(\\theta))$$\n",
        "\n",
        "\n",
        "Then convert it into cartesian coordinates ([howto](http://www.mathsisfun.com/polar-cartesian-coordinates.html)) and plot the results.\n",
        "\n",
        "Use torch tensors only: no lists, loops, numpy arrays, etc."
      ],
      "metadata": {
        "id": "gUtSrsCYaRdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta = torch.linspace(- np.pi, np.pi, steps=1000)\n",
        "\n",
        "# compute rho(theta) as per formula above\n",
        "rho = YOUR CODE HERE\n",
        "\n",
        "# Now convert polar (rho, theta) pairs into cartesian (x,y) to plot them.\n",
        "x = YOUR CODE HERE\n",
        "y = YOUR CODE HERE\n",
        "\n",
        "\n",
        "plt.figure(figsize=[6, 6])\n",
        "plt.fill(x.numpy(), y.numpy(), color='green')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "sTtmnC-EaIr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Going deeper (6 points)\n",
        "\n",
        "Your ultimate task here is to build your first neural network [almost] from scratch and pure PyTorch.\n",
        "\n",
        "This time you will solve the same digit recognition problem, but at a larger scale\n",
        "\n",
        "* 10 different letters\n",
        "* 20k samples\n",
        "\n",
        "We want you to build a network that __reaches at least 80% accuracy__ and has __at least 2 linear layers__ in it.\n",
        "\n",
        "With 10 classes you need __categorical crossentropy__  (see [here](http://wiki.fast.ai/index.php/Log_Loss)) loss. You can write it any way you want, but we recommend to use log_softmax function from pytorch, since it is more numerically stable.\n",
        "\n",
        "Note that you are not required to build 152-layer monsters here. A 2-layer (one hidden, one output) neural network should already give you nice score.\n",
        "\n",
        "__Win conditions:__\n",
        "* __Your model must be nonlinear,__ but not necessarily deep.\n",
        "* __Train your model with your own SGD__ - which you will have to implement\n",
        "* __For this task only, please do not use the contents of `torch.nn` and `torch.optim`.__ That's for the next task.\n",
        "* __Do not use Conv layers__\n",
        "\n",
        "**Bonus:** For the best score in group you get +1.5, 1.0, 0.5 point(1st. 2nd, 3rd places)."
      ],
      "metadata": {
        "id": "vcfxlYBna3Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from notmnist import load_notmnist\n",
        "X_train, y_train, X_val, y_val = load_notmnist(letters='ABCDEFGHIJ')\n",
        "X_train, X_val = X_train.reshape([-1, 784]), X_val.reshape([-1, 784])"
      ],
      "metadata": {
        "id": "_uZn0Bdba3pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.figure(figsize=[12, 4])\n",
        "for i in range(20):\n",
        "    plt.subplot(2, 10, i+1)\n",
        "    plt.imshow(X_train[i].reshape([28, 28]))\n",
        "    plt.title(str(y_train[i]))"
      ],
      "metadata": {
        "id": "3WJlL3PHbs2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
      ],
      "metadata": {
        "id": "_M4n3fcDbvqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = np.unique(y_train)\n",
        "n_classes = len(classes)\n",
        "classes"
      ],
      "metadata": {
        "id": "bIH6GPb3djr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomNet:\n",
        "    def __init__(self, hidden_size, in_size=28*28, num_classes=n_classes):\n",
        "        # self.W = YOUR CODE HERE\n",
        "        pass\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        pass"
      ],
      "metadata": {
        "id": "mc7bSgpCbzHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = CustomNet()\n",
        "out = net.forward(torch.randn(2, 28*28, device=DEVICE))\n",
        "assert len(out.shape) == 2\n",
        "assert out.shape[-1] == n_classes"
      ],
      "metadata": {
        "id": "649qrlZXfUB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def cross_entropy_loss(logits, target):\n",
        "    N = logits.size(0)\n",
        "    # Get the log probabilities\n",
        "    log_probs = # YOUR CODE HERE\n",
        "    # Gather the log probabilities at the target indices\n",
        "    log_probs_at_target = # YOUR CODE HERE\n",
        "    # Compute the negative log likelihood\n",
        "    nll = # YOUR CODE HERE\n",
        "    return nll / N\n",
        "\n",
        "y_tmp = torch.tensor(y_train[:2], device=DEVICE)\n",
        "cross_entropy_loss(out, y_tmp), torch.nn.CrossEntropyLoss()(out, y_tmp)"
      ],
      "metadata": {
        "id": "HnnVGZSwmyu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSGD:\n",
        "    def __init__(self, model, lr=1e-4):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for param in # YOUR CODE HERE:\n",
        "                # YOUR CODE HERE\n",
        "    def zero_grad(self):\n",
        "        for param in # YOUR CODE HERE:\n",
        "            # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "70swrTmCeBZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_minibatches(X, y, batch_size):\n",
        "    indices = np.random.permutation(np.arange(len(X)))\n",
        "    for start in range(0, len(indices), batch_size):\n",
        "        ix = indices[start: start + batch_size]\n",
        "        yield torch.from_numpy(X[ix]), torch.from_numpy(y[ix])"
      ],
      "metadata": {
        "id": "nVvO14e90YjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, optimizer, loss_fn, n_epoch=20):\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    val_loss_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        # Training\n",
        "        # net.train()\n",
        "        acc_batches=[]\n",
        "        loss_batches=[]\n",
        "        for x_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size=64):\n",
        "            # x_batch = # YOUR CODE HERE\n",
        "            # y_batch = # YOUR CODE HERE\n",
        "            # Forward\n",
        "            # loss = # YOUR CODE HERE\n",
        "\n",
        "            # Backward\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "            # Accuracy\n",
        "            acc_batches += (out.argmax(axis=1) == y_batch).detach().cpu().numpy().tolist()\n",
        "\n",
        "        loss_history.append(np.mean(loss_batches))\n",
        "        acc_history.append(np.mean(acc_batches))\n",
        "\n",
        "        # Validating\n",
        "        # net.eval()\n",
        "        with torch.no_grad():\n",
        "            acc_batches=[]\n",
        "            loss_batches=[]\n",
        "            for x_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size=64):\n",
        "                # x_batch = # YOUR CODE HERE\n",
        "                # y_batch = # YOUR CODE HERE\n",
        "                # Forward\n",
        "                # loss = # YOUR CODE HERE\n",
        "                # Accuracy\n",
        "                acc_batches += (out.argmax(axis=1) == y_batch).detach().cpu().numpy().tolist()\n",
        "\n",
        "            val_loss_history.append(np.mean(loss_batches))\n",
        "            val_acc_history.append(np.mean(acc_batches))\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "        ax1.set_xlabel(\"#epoch\")\n",
        "        ax1.set_ylabel(\"Loss\")\n",
        "        ax1.plot(loss_history, 'b', label='train loss')\n",
        "        ax1.plot(val_loss_history, 'r', label='val loss')\n",
        "\n",
        "        ax2.set_xlabel(\"#epoch\")\n",
        "        ax2.set_ylabel(\"Acc\")\n",
        "        ax2.plot(acc_history, 'b', label='train acc')\n",
        "        ax2.plot(val_acc_history, 'r', label='val acc')\n",
        "        plt.axhline(y = 0.8, color = 'g', linestyle = '--')\n",
        "\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    return max(val_acc_history)"
      ],
      "metadata": {
        "id": "DjLxUiA1eLtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = # YOUR CODE HERE\n",
        "opt = # YOUR CODE HERE\n",
        "# train(net, opt, cross_entropy_loss)"
      ],
      "metadata": {
        "id": "Ps2HNwINqtVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hints:\n",
        "  - You'll have to use matrix W(feature_id x class_id)\n",
        "  - Softmax (exp over sum of exps) can be implemented manually or as `torch.softmax`\n",
        "  - Probably better to use STOCHASTIC gradient descent (minibatch) for greater speed\n",
        "  - You need to train both layers, not just the output layer :)\n",
        "  - 50 hidden neurons and a ReLU nonlinearity will do for a start. Many ways to improve.\n",
        "  - In ideal case this totals to 2 `torch.matmul`'s, 1 softmax and 1 ReLU/sigmoid  \n",
        "  - If anything seems wrong, try going through one step of training and printing everything you compute.\n",
        "  - If you see NaNs midway through optimization, you can estimate $\\log P(y \\mid x)$ as `torch.log_softmax(last_linear_layer_outputs)`."
      ],
      "metadata": {
        "id": "wwQYqNdugwmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3. Overfitting (4 points)\n"
      ],
      "metadata": {
        "id": "1fxSuZJwb1bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we work with [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) (*hint: it is available in `torchvision`*).\n",
        "\n",
        "Your goal for today:\n",
        "0. Fill the gaps in training loop and architectures.\n",
        "1. Train a tiny __FC__ network.\n",
        "2. Cause considerable overfitting by modifying the network (e.g. increasing the number of network parameters and/or layers) and demonstrate in in the appropriate way (e.g. plot loss and accurasy on train and validation set w.r.t. network complexity).\n",
        "3. Try to deal with overfitting (at least partially) by using regularization techniques (Dropout/Batchnorm/...) and demonstrate the results.\n",
        "\n",
        "Train a network that achieves $\\geq 0.885$ test accuracy. Again you should use only Linear (`nn.Linear`) layers and activations/dropout/batchnorm. Convolutional layers might be a great use, but we will meet them a bit later.\n",
        "\n",
        "__Please, write a small report describing your ideas, tries and achieved results in the end of this task.__\n",
        "\n",
        "*Note*: in task 3 your goal is to make the network from task 2 less prone to overfitting. And then to train the network that achives $\\geq 0.885$ test accuracy, so it can be different.\n",
        "\n",
        "**Bonus:** For the best score in group you get +1.5, 1.0, 0.5 point(1st, 2nd, 3rd places)."
      ],
      "metadata": {
        "id": "g9j3vEc9rsQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchsummary\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "O1fxVkDOb3QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Technical function\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(root_path):\n",
        "        os.mkdir(root_path)\n",
        "        print('Directory', path, 'is created!')\n",
        "    else:\n",
        "        print('Directory', path, 'already exists!')\n",
        "\n",
        "root_path = 'fmnist'\n",
        "mkdir(root_path)"
      ],
      "metadata": {
        "id": "kzUtrIEgrwWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt6LE7XaTDT9"
      },
      "outputs": [],
      "source": [
        "download = True\n",
        "train_transform = transforms.ToTensor()\n",
        "test_transform = transforms.ToTensor()\n",
        "\n",
        "fmnist_dataset_train = torchvision.datasets.FashionMNIST(root_path,\n",
        "                                                        train=True,\n",
        "                                                        transform=train_transform,\n",
        "                                                        target_transform=None,\n",
        "                                                        download=download)\n",
        "fmnist_dataset_test = torchvision.datasets.FashionMNIST(root_path,\n",
        "                                                       train=False,\n",
        "                                                       transform=test_transform,\n",
        "                                                       target_transform=None,\n",
        "                                                       download=download)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fmnist_dataset_train, fmnist_dataset_val = train_test_split(fmnist_dataset_train, train_size=50000)"
      ],
      "metadata": {
        "id": "15B67G4iGMIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(fmnist_dataset_train), len(fmnist_dataset_val), len(fmnist_dataset_test)"
      ],
      "metadata": {
        "id": "cpbEkSCM1n3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71YP0SPwTIxD"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(fmnist_dataset_train,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(fmnist_dataset_val,\n",
        "                                           batch_size=256,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(fmnist_dataset_test,\n",
        "                                          batch_size=256,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHca15bOTY4B"
      },
      "outputs": [],
      "source": [
        "for img, label in train_loader:\n",
        "    print(img.shape)\n",
        "    # print(img)\n",
        "    print(label.shape)\n",
        "    break\n",
        "\n",
        "plt.imshow(img[0, 0]);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_loop(net, train_loader, val_loader, name, optimizer, criterion, n_epoch=20):\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    val_loss_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        net.train()\n",
        "        acc_batches=[]\n",
        "        loss_batches=[]\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            # x_batch = YOUR CODE HERE\n",
        "            # y_batch = YOUR CODE HERE\n",
        "\n",
        "            # Forward\n",
        "            # loss = YOUR CODE HERE\n",
        "\n",
        "            # Backward\n",
        "            # ... YOUR CODE HERE\n",
        "\n",
        "            # Accuracy\n",
        "            # acc_batches = YOUR CODE HERE\n",
        "\n",
        "        loss_history.append(np.mean(loss_batches))\n",
        "        acc_history.append(np.mean(acc_batches))\n",
        "\n",
        "        # Validating\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            acc_batches=[]\n",
        "            loss_batches=[]\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                # x_batch = YOUR CODE HERE\n",
        "                # y_batch = YOUR CODE HERE\n",
        "\n",
        "                # Forward\n",
        "                # loss = YOUR CODE HERE\n",
        "\n",
        "                # Accuracy\n",
        "                # acc_batches = YOUR CODE HERE\n",
        "\n",
        "            val_loss_history.append(np.mean(loss_batches))\n",
        "            val_acc_history.append(np.mean(acc_batches))\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.title(f\"Training/validating loss {name}\")\n",
        "        plt.xlabel(\"#epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.plot(loss_history, 'b', label='train')\n",
        "        plt.plot(val_loss_history, 'r', label='validation')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.title(f\"Training/validating accuracy {name}\")\n",
        "        plt.xlabel(\"#epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.plot(acc_history, 'b', label='train')\n",
        "        plt.plot(val_acc_history, 'r', label='validation')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "def test_accuracy(model):\n",
        "    model.eval()\n",
        "    test_acc_batches = []\n",
        "    with torch.no_grad():\n",
        "        for X_test, Y_test in test_loader:\n",
        "            X_test = X_test.to(DEVICE)\n",
        "            Y_test = Y_test.to(DEVICE)\n",
        "            out = model.forward(X_test)\n",
        "            test_acc_batches += (out.argmax(axis=1) == Y_test).detach().cpu().numpy().tolist()\n",
        "    print(f'Test accuracy {np.mean(test_acc_batches)}')"
      ],
      "metadata": {
        "id": "QBu9fg_dAymN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6OOOffHTfX5"
      },
      "source": [
        "## Task 3.1 Tiny net\n",
        "Train a tiny network just to validate correctness of train loop, net architecture, params."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftpkTjxlTcFx"
      },
      "outputs": [],
      "source": [
        "class TinyNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(), # This layer converts image into a vector to use Linear layers afterwards\n",
        "            # YOUR CODE HERE\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        out = self.model(inp)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TinyNeuralNetwork()\n",
        "out = model(torch.randn(2, 1, 28, 28))\n",
        "assert len(out.shape) == 2\n",
        "assert out.shape[-1] == 10"
      ],
      "metadata": {
        "id": "bFnYI29v4N1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAhMwySkrlpq"
      },
      "outputs": [],
      "source": [
        "torchsummary.summary(TinyNeuralNetwork().to(DEVICE), (28*28,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3POFj90Ti-6"
      },
      "outputs": [],
      "source": [
        "# tiny_model = TinyNeuralNetwork().to(DEVICE)\n",
        "# opt = # YOUR CODE HERE\n",
        "# loss_func = # YOUR CODE HERE\n",
        "# n_epoch = # YOUR CODE HERE\n",
        "\n",
        "# Your experiments, come here\n",
        "# train_val_loop(tiny_model, train_loader=train_loader, val_loader=val_loader, name='tiny model', optimizer=opt, criterion=loss_func, n_epoch=n_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_accuracy(tiny_model)"
      ],
      "metadata": {
        "id": "XxRsu7pBIf5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7ISqkjmCPB1"
      },
      "source": [
        "## Task 3.2: Overfit it.\n",
        "Build a network that will overfit to this dataset. Demonstrate the overfitting in the appropriate way (e.g. plot loss and accurasy on train and test set w.r.t. network complexity).\n",
        "\n",
        "*Note:* you also might decrease the size of `train` dataset to enforce the overfitting and speed up the computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H12uAWiGBwJx"
      },
      "outputs": [],
      "source": [
        "class OverfittingNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(), # This layer converts image into a vector to use Linear layers afterwards\n",
        "            # YOUR CODE HERE\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        out = self.model(inp)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = OverfittingNeuralNetwork()\n",
        "out = model(torch.randn(2, 1, 28, 28))\n",
        "assert len(out.shape) == 2\n",
        "assert out.shape[-1] == 10"
      ],
      "metadata": {
        "id": "zR4muHZr5k8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgXAKCpvCwqH"
      },
      "outputs": [],
      "source": [
        "torchsummary.summary(OverfittingNeuralNetwork().to(DEVICE), (28*28,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iyuwd4ZLrlpr"
      },
      "outputs": [],
      "source": [
        "# overfit_model = OverfittingNeuralNetwork().to(DEVICE)\n",
        "# opt = # YOUR CODE HERE\n",
        "# loss_func = # YOUR CODE HERE\n",
        "# n_epoch = # YOUR CODE HERE\n",
        "\n",
        "# Your experiments, come here\n",
        "# train_val_loop(overfit_model, train_loader=train_loader, val_loader=val_loader, name='overfit model', optimizer=opt, criterion=loss_func, n_epoch=n_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_accuracy(overfit_model)"
      ],
      "metadata": {
        "id": "GqFQzpfJIem3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG8mNHtPrlpr"
      },
      "source": [
        "## Task 3.3: Fix it.\n",
        "Fix the overfitted network from the previous step (at least partially) by using regularization techniques (Dropout/Batchnorm/...) and demonstrate the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42343iSyrlpr"
      },
      "outputs": [],
      "source": [
        "class FixedNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(), # This layer converts image into a vector to use Linear layers afterwards\n",
        "            # YOUR CODE HERE\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        out = self.model(inp)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FixedNeuralNetwork()\n",
        "out = model(torch.randn(2, 1, 28, 28))\n",
        "assert len(out.shape) == 2\n",
        "assert out.shape[-1] == 10"
      ],
      "metadata": {
        "id": "93Twxz_N6Ade"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR1xQBp9rlps"
      },
      "outputs": [],
      "source": [
        "torchsummary.summary(FixedNeuralNetwork().to(DEVICE), (28*28,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMdEf9Kbrlps"
      },
      "outputs": [],
      "source": [
        "# fixed_model = FixedNeuralNetwork().to(device)\n",
        "# opt = # YOUR CODE HERE\n",
        "# loss_func = # YOUR CODE HERE\n",
        "# n_epoch = # YOUR CODE HERE\n",
        "\n",
        "# Your experiments, come here\n",
        "# train_val_loop(fixed_model, train_loader=train_loader, val_loader=val_loader, name='fixed model', optimizer=opt, criterion=loss_func, n_epoch=n_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_accuracy(fixed_model)"
      ],
      "metadata": {
        "id": "idv0HgvDIckN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMui_uLJ7G0d"
      },
      "source": [
        "### Conclusions:\n",
        "_Write down small report with your conclusions and your ideas._\n",
        "\n",
        "YOUR WORDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4. Your own nn layer. (4 points)"
      ],
      "metadata": {
        "id": "7J7Tpbc9udSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Basically, you can think of a module as of a something (black box)\n",
        "    which can process `input` data and produce `ouput` data.\n",
        "    This is like applying a function which is called `forward`:\n",
        "\n",
        "        output = module.forward(input)\n",
        "\n",
        "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
        "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
        "    The latter implies there is a gradient from previous step of a chain rule.\n",
        "\n",
        "        gradInput = module.backward(input, gradOutput)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "\n",
        "        This includes\n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field.\n",
        "\n",
        "        Make sure to both store the data in `output` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.output = input\n",
        "        # return self.output\n",
        "\n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input.\n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "\n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "\n",
        "        Make sure to both store the gradients in `gradInput` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.gradInput = gradOutput\n",
        "        # return self.gradInput\n",
        "\n",
        "        pass\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Module\""
      ],
      "metadata": {
        "id": "vEb-QueVzvMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear transform layer\n",
        "Also known as dense layer, fully-connected layer, FC-layer.\n",
        "You should implement it.\n",
        "\n",
        "- input:   **`batch_size x n_feats1`**\n",
        "- output: **`batch_size x n_feats2`**"
      ],
      "metadata": {
        "id": "HzohCP4qz42l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    A module which applies a linear transformation\n",
        "    A common name is fully-connected layer, InnerProductLayer in caffe.\n",
        "\n",
        "    The module should work with 2D input of shape (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        # This is a nice initialization\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        #it is important that we should multiply X @ W^T\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "\n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
        "        return q"
      ],
      "metadata": {
        "id": "bbkgSwhpz1yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_Linear():\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    batch_size, n_in, n_out = 2, 3, 4\n",
        "    for _ in range(100):\n",
        "        # layers initialization\n",
        "        torch_layer = torch.nn.Linear(n_in, n_out)\n",
        "        custom_layer = Linear(n_in, n_out)\n",
        "        custom_layer.W = torch_layer.weight.data.numpy()\n",
        "        custom_layer.b = torch_layer.bias.data.numpy()\n",
        "\n",
        "        layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
        "        next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)\n",
        "\n",
        "        # 1. check layer output\n",
        "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
        "        layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
        "        torch_layer_output_var = torch_layer(layer_input_var)\n",
        "        assert np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6)\n",
        "\n",
        "        # 2. check layer input grad\n",
        "        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
        "        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
        "        torch_layer_grad_var = layer_input_var.grad\n",
        "        assert np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6)\n",
        "\n",
        "        # 3. check layer parameters grad\n",
        "        custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
        "        weight_grad = custom_layer.gradW\n",
        "        bias_grad = custom_layer.gradb\n",
        "        torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
        "        torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
        "        assert np.allclose(torch_weight_grad, weight_grad, atol=1e-6)\n",
        "        assert np.allclose(torch_bias_grad, bias_grad, atol=1e-6)"
      ],
      "metadata": {
        "id": "M0PEw9VVugYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "test_Linear()"
      ],
      "metadata": {
        "id": "xOt8kzgwz7Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBPzu7JAauKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}