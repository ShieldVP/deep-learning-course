{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQFp5ncL0fRy"
      },
      "source": [
        "# Weghts & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK_U5e-90jdl"
      },
      "source": [
        "Logging experiments is an important part of serious projects. It makes your results reproducible, easy to visualize, compare and share. Today we will use [Weights & Biases](https://wandb.ai/) library to track our trainings, it is easy to use while doing a lot of stuff automatically.\n",
        "\n",
        "> Weights & Biases is the machine learning platform for developers to build better models faster. Use W&B's lightweight, interoperable tools to quickly track experiments, version and iterate on datasets, evaluate model performance, reproduce models, visualize results and spot regressions, and share findings with colleagues.\n",
        "\n",
        "You need to create an account on [Weights & Biases](https://wandb.ai/) to push [special token](https://wandb.ai/authorize) below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFu4WWcix2yK"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q\n",
        "\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WUO0UYu0l3N"
      },
      "source": [
        "# Part 1. Machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mmk4yqIfV5F"
      },
      "source": [
        "Today we shall compose encoder-decoder neural networks and apply them to the task of machine translation.\n",
        "\n",
        "![img](https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg)\n",
        "_(img: esciencegroup.files.wordpress.com)_\n",
        "\n",
        "\n",
        "Encoder-decoder architectures are about converting anything to anything, including\n",
        " * Machine translation and spoken dialogue systems\n",
        " * [Image captioning](http://mscoco.org/dataset/#captions-challenge2015) and [image2latex](https://openai.com/requests-for-research/#im2latex) (convolutional encoder, recurrent decoder)\n",
        " * Generating [images by captions](https://arxiv.org/abs/1511.02793) (recurrent encoder, convolutional decoder)\n",
        " * Grapheme2phoneme - convert words to transcripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQXFrpM1fV5F"
      },
      "source": [
        "We gonna try our encoder-decoder models on russian to english machine translation problem. More specifically, we'll translate hotel and hostel descriptions. This task shows the scale of machine translation while not requiring you to train your model for weeks if you don't use GPU.\n",
        "\n",
        "Before we get to the architecture, there's some preprocessing to be done. ~~Go tokenize~~ Alright, this time we've done preprocessing for you. As usual, the data will be tokenized with WordPunctTokenizer.\n",
        "\n",
        "However, there's one more thing to do. Our data lines contain unique rare words. If we operate on a word level, we will have to deal with large vocabulary size. If instead we use character-level models, it would take lots of iterations to process a sequence. This time we're gonna pick something inbetween.\n",
        "\n",
        "One popular approach is called [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt) aka __BPE__. The algorithm starts with a character-level tokenization and then iteratively merges most frequent pairs for N iterations. This results in frequent words being merged into a single token and rare words split into syllables or even characters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUGFdIOWfV5G"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch>=1.3.0 subword-nmt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdAa2Un5fV5G"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/yy2zqh34dyhv07i/data.txt?dl=1 -O data.txt\n",
        "!wget https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/week04_seq2seq/vocab.py -O vocab.py\n",
        "# thanks to tilda and deephack teams for the data, Dmitry Emelyanenko for the code :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH8cZdpGfV5H"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from subword_nmt.learn_bpe import learn_bpe\n",
        "from subword_nmt.apply_bpe import BPE\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "def tokenize(x):\n",
        "    return ' '.join(tokenizer.tokenize(x.lower()))\n",
        "\n",
        "# split and tokenize the data\n",
        "with open('train.en', 'w') as f_src,  open('train.ru', 'w') as f_dst:\n",
        "    for line in open('data.txt'):\n",
        "        src_line, dst_line = line.strip().split('\\t')\n",
        "        f_src.write(tokenize(src_line) + '\\n')\n",
        "        f_dst.write(tokenize(dst_line) + '\\n')\n",
        "\n",
        "# build and apply bpe vocs\n",
        "bpe = {}\n",
        "for lang in ['en', 'ru']:\n",
        "    learn_bpe(open('./train.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)\n",
        "    bpe[lang] = BPE(open('./bpe_rules.' + lang))\n",
        "\n",
        "    with open('train.bpe.' + lang, 'w') as f_out:\n",
        "        for line in open('train.' + lang):\n",
        "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGoosEKMfV5H"
      },
      "source": [
        "### Building vocabularies\n",
        "\n",
        "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yyVcqtCfV5H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHN92bBzfV5H"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_inp = np.array(open('./train.bpe.ru').read().split('\\n'))\n",
        "data_out = np.array(open('./train.bpe.en').read().split('\\n'))\n",
        "\n",
        "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,\n",
        "                                                          random_state=42)\n",
        "for i in range(3):\n",
        "    print('inp:', train_inp[i])\n",
        "    print('out:', train_out[i], end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSbjwWD-fV5H"
      },
      "outputs": [],
      "source": [
        "from vocab import Vocab\n",
        "\n",
        "inp_voc = Vocab.from_lines(train_inp)\n",
        "out_voc = Vocab.from_lines(train_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56zqaCSQfV5I"
      },
      "outputs": [],
      "source": [
        "# Here's how you cast lines into ids and backwards.\n",
        "batch_lines = sorted(train_inp, key=len)[5:10]\n",
        "batch_ids = inp_voc.to_matrix(batch_lines)\n",
        "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
        "\n",
        "print(\"lines\")\n",
        "print(batch_lines)\n",
        "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
        "print(batch_ids)\n",
        "print(\"\\nback to words\")\n",
        "print(batch_lines_restored)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpOke8GefV5I"
      },
      "source": [
        "Draw source and translation length distributions to estimate the scope of the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iPkOH8YfV5I"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[8, 4])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"source length\")\n",
        "plt.hist(list(map(len, map(str.split, train_inp))), bins=20);\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"translation length\")\n",
        "plt.hist(list(map(len, map(str.split, train_out))), bins=20);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwuA1GimfV5I"
      },
      "source": [
        "### Encoder-decoder model (1 point)\n",
        "\n",
        "The code below contains a template for a simple encoder-decoder model: single GRU encoder/decoder, no attention or anything. This model is implemented for you as a reference and a baseline for your homework assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZUQIzIFfV5I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOHQoDz_fV5I"
      },
      "outputs": [],
      "source": [
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
        "        \"\"\"\n",
        "        A simple encoder-decoder seq2seq model\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, parameters, etc.\n",
        "\n",
        "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
        "        self.hid_size = hid_size\n",
        "\n",
        "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
        "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
        "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
        "\n",
        "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
        "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
        "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
        "\n",
        "    def forward(self, inp, out):\n",
        "        \"\"\" Apply model in training mode \"\"\"\n",
        "        initial_state = self.encode(inp)\n",
        "        return self.decode(initial_state, out)\n",
        "\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :returns: initial decoder state tensors, one or many\n",
        "        \"\"\"\n",
        "        inp_emb = self.emb_inp(inp)\n",
        "        batch_size = inp.shape[0]\n",
        "\n",
        "        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)\n",
        "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
        "\n",
        "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
        "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
        "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
        "        # ^-- shape: [batch_size, hid_size]\n",
        "\n",
        "        dec_start = self.dec_start(last_state)\n",
        "        return [dec_start]\n",
        "\n",
        "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
        "        :param prev_state: a list of previous decoder state tensors, same as returned by encode(...)\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch, len(out_voc)]\n",
        "        \"\"\"\n",
        "        prev_gru0_state = prev_state[0]\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        return [new_dec_state], output_logits\n",
        "\n",
        "    def decode(self, initial_state, out_tokens, **flags):\n",
        "        \"\"\" Iterate over reference tokens (out_tokens) with decode_step \"\"\"\n",
        "        batch_size = out_tokens.shape[0]\n",
        "        state = initial_state\n",
        "\n",
        "        # initial logits: always predict BOS\n",
        "        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),\n",
        "                               num_classes=len(self.out_voc)).to(device=out_tokens.device)\n",
        "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
        "\n",
        "        logits_sequence = [first_logits]\n",
        "        for i in range(out_tokens.shape[1] - 1):\n",
        "            state, logits = self.decode_step(state, out_tokens[:, i])\n",
        "            logits_sequence.append(logits)\n",
        "        return torch.stack(logits_sequence, dim=1)\n",
        "\n",
        "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
        "        \"\"\" Generate translations from model (greedy version) \"\"\"\n",
        "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
        "        state = initial_state\n",
        "        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64,\n",
        "                              device=device)]\n",
        "        all_states = [initial_state]\n",
        "\n",
        "        for i in range(max_len):\n",
        "            state, logits = self.decode_step(state, outputs[-1])\n",
        "            outputs.append(logits.argmax(dim=-1))\n",
        "            all_states.append(state)\n",
        "\n",
        "        return torch.stack(outputs, dim=1), all_states\n",
        "\n",
        "    def translate_lines(self, inp_lines, **kwargs):\n",
        "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
        "        initial_state = self.encode(inp)\n",
        "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
        "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbm02hSBfV5I"
      },
      "outputs": [],
      "source": [
        "# debugging area\n",
        "model = BasicModel(inp_voc, out_voc).to(device)\n",
        "\n",
        "dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)\n",
        "dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)\n",
        "\n",
        "h0 = model.encode(dummy_inp_tokens)\n",
        "h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))\n",
        "\n",
        "assert isinstance(h1, list) and len(h1) == len(h0)\n",
        "assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])\n",
        "assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))\n",
        "\n",
        "logits_seq = model.decode(h0, dummy_out_tokens)\n",
        "assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))\n",
        "\n",
        "# full forward\n",
        "logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)\n",
        "assert logits_seq2.shape == logits_seq.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMp3SVk1fV5I"
      },
      "outputs": [],
      "source": [
        "dummy_translations, dummy_states = model.translate_lines(train_inp[:3], max_len=25)\n",
        "print(\"Translations without training:\")\n",
        "print('\\n'.join([line for line in dummy_translations]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fmqtd70fV5I"
      },
      "source": [
        "### Training loss (1 point)\n",
        "\n",
        "Our training objective is almost the same as it was for neural language models:\n",
        "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
        "\n",
        "where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but excluding PAD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6Ce_q8jfV5I"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model, inp, out, **flags):\n",
        "    \"\"\"\n",
        "    Compute loss (float32 scalar) as in the formula above\n",
        "    :param inp: input tokens matrix, int32[batch, time]\n",
        "    :param out: reference tokens matrix, int32[batch, time]\n",
        "\n",
        "    In order to pass the tests, your function should\n",
        "    * include loss at first EOS but not the subsequent ones\n",
        "    * divide sum of losses by a sum of input lengths (use voc.compute_mask)\n",
        "    \"\"\"\n",
        "\n",
        "    mask = model.out_voc.compute_mask(out) # [batch_size, out_len]\n",
        "    targets_one_hot = F.one_hot(out, len(model.out_voc)).to(torch.float32)\n",
        "\n",
        "    # outputs of the model, [batch_size, out_len, num_tokens]\n",
        "    logits_seq = model(inp, out)\n",
        "\n",
        "    # log-probabilities of all tokens at all steps, [batch_size, out_len, num_tokens]\n",
        "    logprobs_seq = torch.log_softmax(logits_seq, dim=-1)\n",
        "\n",
        "    # log-probabilities of correct outputs, [batch_size, out_len]\n",
        "    logp_out = (logprobs_seq * targets_one_hot).sum(dim=-1)\n",
        "\n",
        "    # average cross-entropy over tokens where mask == True\n",
        "    return # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txnFG7OxfV5I"
      },
      "outputs": [],
      "source": [
        "dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens)\n",
        "print(\"Loss:\", dummy_loss)\n",
        "assert np.allclose(dummy_loss.item(), 7.5, rtol=0.1, atol=0.1), \"We're sorry for your loss\"\n",
        "\n",
        "# test autograd\n",
        "dummy_loss.backward()\n",
        "for name, param in model.named_parameters():\n",
        "    assert param.grad is not None and abs(param.grad.max()) != 0, f\"Param {name} received no gradients\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "826wQnN3fV5I"
      },
      "source": [
        "### Evaluation: BLEU\n",
        "\n",
        "Machine translation is commonly evaluated with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. This metric simply computes which fraction of predicted n-grams is actually present in the reference translation. It does so for n=1,2,3 and 4 and computes the geometric average with penalty if translation is shorter than reference.\n",
        "\n",
        "While BLEU [has many drawbacks](http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf), it still remains the most commonly used metric and one of the simplest to compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZcVnJPUfV5J"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
        "    \"\"\"\n",
        "    Estimates corpora-level BLEU score of model's translations given inp and reference out\n",
        "    Note: if you're serious about reporting your results, use https://pypi.org/project/sacrebleu\n",
        "    \"\"\"\n",
        "    with torch.inference_mode():\n",
        "        translations, _ = model.translate_lines(inp_lines, **flags)\n",
        "        translations = [line.replace(bpe_sep, '') for line in translations]\n",
        "        actual = [line.replace(bpe_sep, '') for line in out_lines]\n",
        "        return corpus_bleu(\n",
        "            [[ref.split()] for ref in actual],\n",
        "            [trans.split() for trans in translations],\n",
        "            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
        "        ) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0GCtzOtfV5J"
      },
      "outputs": [],
      "source": [
        "compute_bleu(model, dev_inp, dev_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aClINDZHfV5J"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Training encoder-decoder models isn't that different from any other models: sample batches, compute loss, backprop and update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC0He6z0fV5J"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm, trange\n",
        "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
        "\n",
        "model = BasicModel(inp_voc, out_voc).to(device)\n",
        "learning_rate = 1e-3\n",
        "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mustA9E-fV5J"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(project=\"encoder-decoder\")\n",
        "config = run.config\n",
        "config.learning_rate = learning_rate\n",
        "config.batch_size = batch_size\n",
        "\n",
        "for _ in trange(25_000):\n",
        "    step = len(metrics['train_loss']) + 1\n",
        "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
        "    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
        "    batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
        "\n",
        "    # YOUR CODE: training step using batch_inp and batch_out\n",
        "\n",
        "    metrics['train_loss'].append((step, loss_t.item()))\n",
        "    run.log({\"loss\": loss_t.item()})\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        bleu = compute_bleu(model, dev_inp, dev_out)\n",
        "        metrics['dev_bleu'].append((step, bleu))\n",
        "        run.log({\"bleu\": bleu})\n",
        "\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(12,4))\n",
        "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
        "            plt.subplot(1, len(metrics), i + 1)\n",
        "            plt.title(name)\n",
        "            plt.plot(*zip(*history))\n",
        "            plt.grid()\n",
        "        plt.show()\n",
        "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
        "run.finish()\n",
        "# Note: it's okay if bleu oscillates up and down as long as it gets better on average over long term (e.g. 5k batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAOSlASjfV5J"
      },
      "outputs": [],
      "source": [
        "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 15, \"We kind of need a higher bleu BLEU from you. Kind of right now.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL0J1etHfV5J"
      },
      "outputs": [],
      "source": [
        "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
        "    print(inp_line)\n",
        "    print(trans_line)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB90WYCifV5J"
      },
      "source": [
        "### Your Attention Required\n",
        "\n",
        "In this section we want you to improve over the basic model by implementing a simple attention mechanism.\n",
        "\n",
        "This is gonna be a two-parter: building the __attention layer__ and using it for an __attentive seq2seq model__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvDmW5XefV5J"
      },
      "source": [
        "### Attention layer (3 points)\n",
        "\n",
        "Here you will have to implement a layer that computes a simple additive attention:\n",
        "\n",
        "Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$ and a single decoder state $h^d$,\n",
        "\n",
        "* Compute logits with a 2-layer neural network\n",
        "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
        "* Get probabilities from logits,\n",
        "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
        "\n",
        "* Add up encoder states with probabilities to get __attention response__\n",
        "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n",
        "\n",
        "You can learn more about attention layers in the lecture slides or [from this post](https://distill.pub/2016/augmented-rnns/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR1D_s-sfV5J"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, enc_size, dec_size, hid_size, activ=torch.tanh):\n",
        "        \"\"\" A layer that computes additive attention response and weights \"\"\"\n",
        "        super().__init__()\n",
        "        self.enc_size = enc_size # num units in encoder state\n",
        "        self.dec_size = dec_size # num units in decoder state\n",
        "        self.hid_size = hid_size # attention layer hidden units\n",
        "        self.activ = activ       # attention layer hidden nonlinearity\n",
        "\n",
        "        # create trainable paramteres like this:\n",
        "        self.<PARAMETER_NAME> = nn.Parameter(<INITIAL_VALUES>, requires_grad=True)\n",
        "        <...>  # you will need a couple of these\n",
        "\n",
        "\n",
        "    def forward(self, enc, dec, inp_mask):\n",
        "        \"\"\"\n",
        "        Computes attention response and weights\n",
        "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
        "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
        "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
        "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
        "            - attn - attention response vector (weighted sum of enc)\n",
        "            - probs - attention weights after softmax\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute logits\n",
        "        <...>\n",
        "\n",
        "        # Apply mask - if mask is 0, logits should be -inf or -1e9\n",
        "        # You may need torch.where\n",
        "        <...>\n",
        "\n",
        "        # Compute attention probabilities (softmax)\n",
        "        probs = <...>\n",
        "\n",
        "        # Compute attention response using enc and probs\n",
        "        attn = <...>\n",
        "\n",
        "        return attn, probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdUNtlbtfV5J"
      },
      "source": [
        "### Seq2seq model with attention (3 points)\n",
        "\n",
        "You can now use the attention layer to build a network. The simplest way to implement attention is to use it in decoder phase:\n",
        "\n",
        "![img](https://i.imgur.com/6fKHlHb.png)\n",
        "\n",
        "_image from distill.pub [article](https://distill.pub/2016/augmented-rnns/)_\n",
        "\n",
        "On every step, use __previous__ decoder state to obtain attention response. Then feed concat this response to the inputs of next attention layer.\n",
        "\n",
        "The key implementation detail here is __model state__. Put simply, you can add any tensor into the list of `encode` outputs. You will then have access to them at each `decode` step. This may include:\n",
        "* Last RNN hidden states (as in basic model)\n",
        "* The whole sequence of encoder outputs (to attend to) and mask\n",
        "* Attention probabilities (to visualize)\n",
        "\n",
        "_There are, of course, alternative ways to wire attention into your network and different kinds of attention. Take a look at [this](https://arxiv.org/abs/1609.08144), [this](https://arxiv.org/abs/1706.03762) and [this](https://arxiv.org/abs/1808.03867) for ideas. And for image captioning/im2latex there's [visual attention](https://arxiv.org/abs/1502.03044)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nnMRDckfV5J"
      },
      "outputs": [],
      "source": [
        "class AttentiveModel(BasicModel):\n",
        "    def __init__(self, name, inp_voc, out_voc,\n",
        "                 emb_size=64, hid_size=128, attn_size=128):\n",
        "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
        "        nn.Module.__init__(self)  # initialize base class to track sub-layers, trainable variables, etc.\n",
        "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
        "        self.hid_size = hid_size\n",
        "\n",
        "        # YOUR CODE: initialize layers\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :return: a list of initial decoder state tensors\n",
        "        \"\"\"\n",
        "\n",
        "        # encode input sequence, create initial decoder states\n",
        "        # YOUR CODE\n",
        "\n",
        "        # apply attention layer from initial decoder hidden state\n",
        "        first_attn_probas = <...>\n",
        "\n",
        "        # Build first state: include\n",
        "        # * initial states for decoder recurrent layers\n",
        "        # * encoder sequence and encoder attn mask (for attention)\n",
        "        # * make sure that last state item is attention probabilities tensor\n",
        "\n",
        "        first_state = [<...>, first_attn_probas]\n",
        "        return first_state\n",
        "\n",
        "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
        "        :param prev_state: a list of previous decoder state tensors\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
        "        \"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return [new_dec_state, output_logits]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFwSs5vufV5L"
      },
      "source": [
        "### Training attentive model\n",
        "\n",
        "Please reuse the infrastructure you've built for the regular model. I hope you didn't hard-code anything :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn_wNnlpfV5L"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE: create AttentiveModel and training utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6Qe-UvGfV5O"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE: training loop (don't forget to log metrics in wandb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F4TNn7NfV5O"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE: measure final BLEU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEGFFFbBfV5P"
      },
      "source": [
        "### Visualizing model attention (1 point)\n",
        "\n",
        "After training the attentive translation model, you can check it's sanity by visualizing its attention weights.\n",
        "\n",
        "We provided you with a function that draws attention maps using [`Bokeh`](https://bokeh.pydata.org/en/latest/index.html). Once you managed to produce something better than random noise, please save at least 3 attention maps and __submit them to anytask__ alongside this notebook to get the max grade. Saving bokeh figures as __cell outputs is not enough!__ (TAs can't see saved bokeh figures in anytask). You can save bokeh images as screenshots or using this button:\n",
        "\n",
        "![bokeh_panel](https://github.com/yandexdataschool/nlp_course/raw/2019/resources/bokeh_panel.png)\n",
        "\n",
        "__Note:__ you're not locked into using bokeh. If you prefer a different visualization method, feel free to use that instead of bokeh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrcHmGAjfV5P"
      },
      "outputs": [],
      "source": [
        "import bokeh.plotting as pl\n",
        "import bokeh.models as bm\n",
        "from bokeh.io import output_notebook, show\n",
        "output_notebook()\n",
        "\n",
        "def draw_attention(inp_line, translation, probs):\n",
        "    \"\"\" An intentionally ambiguous function to visualize attention weights \"\"\"\n",
        "    inp_tokens = inp_voc.tokenize(inp_line)\n",
        "    trans_tokens = out_voc.tokenize(translation)\n",
        "    probs = probs[:len(trans_tokens), :len(inp_tokens)]\n",
        "\n",
        "    fig = pl.figure(x_range=(0, len(inp_tokens)), y_range=(0, len(trans_tokens)),\n",
        "                    x_axis_type=None, y_axis_type=None, tools=[])\n",
        "    fig.image([probs[::-1]], 0, 0, len(inp_tokens), len(trans_tokens))\n",
        "\n",
        "    fig.add_layout(bm.LinearAxis(axis_label='source tokens'), 'above')\n",
        "    fig.xaxis.ticker = np.arange(len(inp_tokens)) + 0.5\n",
        "    fig.xaxis.major_label_overrides = dict(zip(np.arange(len(inp_tokens)) + 0.5, inp_tokens))\n",
        "    fig.xaxis.major_label_orientation = 45\n",
        "\n",
        "    fig.add_layout(bm.LinearAxis(axis_label='translation tokens'), 'left')\n",
        "    fig.yaxis.ticker = np.arange(len(trans_tokens)) + 0.5\n",
        "    fig.yaxis.major_label_overrides = dict(zip(np.arange(len(trans_tokens)) + 0.5, trans_tokens[::-1]))\n",
        "\n",
        "    show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_7DGe4SfV5P"
      },
      "outputs": [],
      "source": [
        "inp = dev_inp[::500]\n",
        "\n",
        "trans, states = attentive_model.translate_lines(inp)\n",
        "\n",
        "# select attention probs from model state (you may need to change this for your custom model)\n",
        "# attention_probs below must have shape [batch_size, translation_length, input_length], extracted from states\n",
        "# e.g. if attention probs are at the end of each state, use np.stack([state[-1] for state in states], axis=1)\n",
        "attention_probs = <YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CesksMIFfV5P"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    draw_attention(inp[i], trans[i], attention_probs[i])\n",
        "\n",
        "# Does it look fine already? don't forget to save images for anytask!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DxIt9VtfV5P"
      },
      "source": [
        "__Note:__ If the attention maps are not iterpretable, try starting encoder from zeros (instead of dec_start), forcing model to use attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-WX_-orOGT5"
      },
      "source": [
        "# Part 2. Named Entity Recognition\n",
        "Here we're gonna solve the problem of named entity recognition. Here's what it does in one picture:\n",
        "\n",
        "![nlp-task-named-entity-recognition.png](https://mobidev.biz/wp-content/uploads/2019/12/nlp-task-named-entity-recognition.png)\n",
        "\n",
        "For each word, in a sentence, your model should predict a named entity class: person, organization, location and so on\n",
        "\n",
        "In this part of assignment we will use a bit higher level DL interface, we won't write any loops and etc, but we will do some work around the data (and use LoRA for fine-tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0802SPERrHA"
      },
      "source": [
        "## Dataset exploration (0.5 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USV5lSUV5cF5"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDrFJrRu_ecb"
      },
      "source": [
        "We will work with [multinerd](https://huggingface.co/datasets/Babelscape/multinerd) dataset from the ðŸ¤— HuggingFace hub, specifically with its russian subset. NB: dataset was created automatically, not manualy, therefore it may contain some labeling mistakes obvious for people. It is based on wiki sentences and has 15 NER categories:\n",
        "- Person (PER)\n",
        "- Location (LOC)\n",
        "- Organization (ORG)\n",
        "- Animal (ANIM)\n",
        "- Biological entity (BIO)\n",
        "- Celestial Body (CEL)\n",
        "- Disease (DIS)\n",
        "- Event (EVE)\n",
        "- Food (FOOD)\n",
        "- Instrument (INST)\n",
        "- Media (MEDIA)\n",
        "- Plant (PLANT)\n",
        "- Mythological entity (MYTH)\n",
        "- Time (TIME)\n",
        "- Vehicle (VEHI)\n",
        "\n",
        "The format of all dataset follows popular [IOB format](https://en.wikipedia.org/wiki/Insideâ€“outsideâ€“beginning_(tagging)). The B- prefix before a tag indicates that the tag is the beginning of a chunk, and an I- prefix before a tag indicates that the tag is inside a chunk. The B- tag is used only when a tag is followed by a tag of the same type without O tokens between them. An O tag indicates that a token belongs to no chunk. Therefore there we have 31 named entities.\n",
        "\n",
        "To load the dataset, we use the `load_dataset()` method from the ðŸ¤— Datasets library (don't worry about three last progress bars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03b6NQ3C-34p"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"Babelscape/multinerd\", verification_mode=\"no_checks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4qEaHyM94Vb"
      },
      "outputs": [],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwVMKBOpAUx5"
      },
      "outputs": [],
      "source": [
        "raw_datasets['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmVmIxzuALeS"
      },
      "source": [
        "Filter dataset by language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh2lYHe599Lz"
      },
      "outputs": [],
      "source": [
        "for dataset_name in raw_datasets:\n",
        "    raw_datasets[dataset_name] = raw_datasets[dataset_name].filter(<YOUR CODE>)\n",
        "    raw_datasets[dataset_name] = raw_datasets[dataset_name].remove_columns('lang')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0myEtRHAfT2"
      },
      "outputs": [],
      "source": [
        "assert len(raw_datasets['train']) == 66240\n",
        "assert len(raw_datasets['validation']) == 8280\n",
        "assert len(raw_datasets['test']) == 8338"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-0bQjN6-wNJ"
      },
      "outputs": [],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD0w6fQf-Hoq"
      },
      "outputs": [],
      "source": [
        "raw_datasets['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If6OzzJ_F-B-"
      },
      "source": [
        "Let's see the `ner_tags` description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec4RE7XV-eaf"
      },
      "outputs": [],
      "source": [
        "raw_datasets['train'].features['ner_tags']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8xSz1YHGD7e"
      },
      "source": [
        "It is just indices, so to make it readable we should apply [mapping](https://huggingface.co/datasets/Babelscape/multinerd#dataset-structure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBS54EeC_dgV"
      },
      "outputs": [],
      "source": [
        "tag2idx = {\n",
        "    \"O\": 0,\n",
        "    \"B-PER\": 1,\n",
        "    \"I-PER\": 2,\n",
        "    \"B-ORG\": 3,\n",
        "    \"I-ORG\": 4,\n",
        "    \"B-LOC\": 5,\n",
        "    \"I-LOC\": 6,\n",
        "    \"B-ANIM\": 7,\n",
        "    \"I-ANIM\": 8,\n",
        "    \"B-BIO\": 9,\n",
        "    \"I-BIO\": 10,\n",
        "    \"B-CEL\": 11,\n",
        "    \"I-CEL\": 12,\n",
        "    \"B-DIS\": 13,\n",
        "    \"I-DIS\": 14,\n",
        "    \"B-EVE\": 15,\n",
        "    \"I-EVE\": 16,\n",
        "    \"B-FOOD\": 17,\n",
        "    \"I-FOOD\": 18,\n",
        "    \"B-INST\": 19,\n",
        "    \"I-INST\": 20,\n",
        "    \"B-MEDIA\": 21,\n",
        "    \"I-MEDIA\": 22,\n",
        "    \"B-MYTH\": 23,\n",
        "    \"I-MYTH\": 24,\n",
        "    \"B-PLANT\": 25,\n",
        "    \"I-PLANT\": 26,\n",
        "    \"B-TIME\": 27,\n",
        "    \"I-TIME\": 28,\n",
        "    \"B-VEHI\": 29,\n",
        "    \"I-VEHI\": 30,\n",
        "}\n",
        "\n",
        "idx2tag = {v: k for k, v in tag2idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbfuvEFGKDMz"
      },
      "source": [
        "There is a fancy visualizer in `spacy` nlp library we can adapt for custom dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieNINSNnF7gz"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "import typing as tp\n",
        "\n",
        "def ner_render(tokens: tp.Sequence[str], ner_tags: tp.Sequence[int], title: tp.Optional[str] = None, **kwargs):\n",
        "    pos = 0\n",
        "    ents = []\n",
        "    for word, tag_ in zip(tokens, ner_tags):\n",
        "        tag = idx2tag[tag_]\n",
        "        if tag.startswith('B'):\n",
        "            ents.append({\n",
        "                \"start\": pos,\n",
        "                \"end\": pos + len(word),\n",
        "                \"label\": tag.split(\"-\")[1]\n",
        "            })\n",
        "        elif tag.startswith('I'):\n",
        "            ents[-1][\"end\"] = pos + len(word)\n",
        "        pos += (len(word) + 1)\n",
        "    displacy.render({\n",
        "        \"text\": \" \".join(tokens),\n",
        "        \"ents\": ents,\n",
        "        \"title\": title\n",
        "    }, style=\"ent\", manual=True, jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63L2CddIKpwk"
      },
      "source": [
        "Et voilÃ !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2SZBTTyHhpU"
      },
      "outputs": [],
      "source": [
        "for train_id in [50, 150, 300]:\n",
        "    ner_render(**raw_datasets['train'][train_id], title = f'multinerd train[{train_id}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZCVgJVdLAAA"
      },
      "source": [
        "Let's count each type of tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnOn_He9IM1B"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "from collections import Counter\n",
        "\n",
        "tag_counter = Counter()\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_WTdnrLL1V2"
      },
      "outputs": [],
      "source": [
        "assert tag_counter[0] > (sum(tag_counter.values()) - tag_counter[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "996VhBiXLYqX"
      },
      "outputs": [],
      "source": [
        "for idx, cntr in sorted(tag_counter.items(), key=lambda kv: kv[1], reverse=True):\n",
        "    print(idx2tag[idx], cntr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zneCfRSMWJOQ"
      },
      "source": [
        "## Dataset preparation (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mJ3jldubhrU"
      },
      "source": [
        "It is time to tokenize out texts. We will use tokenizer of pretrained BERT. You can try [any model](https://huggingface.co/models) you want, but remember we will deal russian texts mostly (it is better if text splits in fewer amount of tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPrcPZskWhIO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"DeepPavlov/rubert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVBehIcpcM-a"
      },
      "source": [
        "To tokenize a pre-tokenized input, we can use our `tokenizer` as usual and just add `is_split_into_words=True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwNGBKTVL68p"
      },
      "outputs": [],
      "source": [
        "example = raw_datasets[\"train\"][0]\n",
        "inputs = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "print(len(inputs.tokens()), inputs.tokens())\n",
        "print(len(example[\"ner_tags\"]), example[\"ner_tags\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFSn6ycHcKZI"
      },
      "source": [
        "As we can see, the tokenizer added the special tokens used by the model (`[CLS]` at the beginning and `[SEP]` at the end) and left most of the words untouched. The word `Ð½Ð°Ð¿Ð¾Ð¼Ð¸Ð½Ð°Ð²ÑˆÐ¸Ðµ`, however, was tokenized into two subwords, `Ð½Ð°Ð¿Ð¾Ð¼Ð¸Ð½Ð°` and `##Ð²ÑˆÐ¸Ðµ`. This introduces a mismatch between our inputs and the labels: the list of labels has only 19 elements, whereas our input now has 24 tokens. Accounting for the special tokens is easy (we know they are at the beginning and the end), but we also need to make sure we align all the labels with the proper words.\n",
        "\n",
        "To handle it we should know the relations between tokens and words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x6nU_WZarEF"
      },
      "outputs": [],
      "source": [
        "print(len(inputs.word_ids()), inputs.word_ids())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3YlyvjgdxpK"
      },
      "source": [
        "With a tiny bit of work, we can then expand our label list to match the tokens:\n",
        "- The first rule weâ€™ll apply is that special tokens get a label of $-100$. This is because by default $-100$ is an index that is ignored in the loss function we will use (cross entropy)\n",
        "- Secondly, each token gets the same label as the token that started the word itâ€™s inside, since they are part of the same entity\n",
        "- And the last case, for tokens inside a word but not at the beginning, we replace the B- with I- (since the token does not begin the entity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du2CfqLNeZR4"
      },
      "outputs": [],
      "source": [
        "def align_labels_with_tokens(labels: tp.Sequence[int], word_ids: tp.Sequence[int]):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:  # start of a new word\n",
        "            # YOUR CODE HERE\n",
        "        elif word_id is None:        # special token\n",
        "            # YOUR CODE HERE\n",
        "        else:                        # same word as previous token\n",
        "            # YOUR CODE HERE\n",
        "        new_labels.append(label)\n",
        "\n",
        "    return new_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBD3v1fLfhOV"
      },
      "outputs": [],
      "source": [
        "for idx in [42, 4224, 4242]:\n",
        "    row = raw_datasets[\"train\"][idx]\n",
        "    labels = row[\"ner_tags\"]\n",
        "\n",
        "    inputs = tokenizer(row[\"tokens\"], is_split_into_words=True)\n",
        "    word_ids = inputs.word_ids()\n",
        "    aligned_labels = align_labels_with_tokens(labels, word_ids)\n",
        "    assert len(inputs.tokens()) == len(aligned_labels)\n",
        "    assert aligned_labels[0] == -100\n",
        "    assert aligned_labels[-1] == -100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCP-inOpfTeF"
      },
      "source": [
        "Letâ€™s try it out on our example sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84aIgiKSeznc"
      },
      "outputs": [],
      "source": [
        "labels = example[\"ner_tags\"]\n",
        "word_ids = tokenizer(example[\"tokens\"], is_split_into_words=True).word_ids()\n",
        "aligned_labels = align_labels_with_tokens(labels, word_ids)\n",
        "print(len(labels), labels)\n",
        "print(len(aligned_labels), aligned_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwj34mNWigLF"
      },
      "source": [
        "To preprocess our whole dataset, we need to tokenize all the inputs and apply `align_labels_with_tokens()` on all the labels. To take advantage of the speed of our tokenizer, itâ€™s best to tokenize lots of texts at the same time, so weâ€™ll write a function that processes a list of examples and use the `Dataset.map()` method with the option `batched=True`. The only thing that is different from our previous example is that the `word_ids()` function needs to get the index of the example we want the word IDs of when the inputs to the tokenizer are lists of texts (or in our case, list of lists of words), so we add that too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWoNoTBKi4ih"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(rows: tp.Sequence[tp.Dict[str, tp.Any]]):\n",
        "    all_labels = rows[\"ner_tags\"]\n",
        "    tokenized_inputs = tokenizer(rows[\"tokens\"], is_split_into_words=True)\n",
        "\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V9zHMM0itS4"
      },
      "source": [
        "We can now apply all that preprocessing in one go on the other splits of our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKbrDxUgjhLa"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDr1z-gjkfmi"
      },
      "source": [
        "Now our datasets look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJpELpJSkJHJ"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9bigJo7kt7J"
      },
      "source": [
        "Here:\n",
        "\n",
        "`input_ids` - indices of input sequence tokens in the vocabulary\n",
        "\n",
        "`token_type_ids` - segment token indices to indicate first and second portions of the inputs in NSP (Next Sentence Prediction) task. Indices are selected in [0, 1]: 0 corresponds to a sentence A token, 1 corresponds to a sentence B token. We don't need it today\n",
        "\n",
        "`attention_mask` - mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPYNehoZu-o0"
      },
      "source": [
        "That's all with preprocessing. To be honest with you, it was the hardest part ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CfAwAz0lFC-"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgV8SuDSppKU"
      },
      "source": [
        "As our datasets are preprocessed we should create `data_collator` -- special function, which takes list of preprocessed sentences and return tensors (padding smaller sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qU-2yuclELc"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHQt50UIqSLt"
      },
      "source": [
        "To test this on a few samples, we can just call it on a list of examples from our tokenized training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmOSecpHqWeu"
      },
      "outputs": [],
      "source": [
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(4)])\n",
        "batch[\"labels\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj3gMayNqnsX"
      },
      "source": [
        "Letâ€™s compare this to the labels in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rgjE48kqgWr"
      },
      "outputs": [],
      "source": [
        "for i in range(4):\n",
        "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgWEyX65quw9"
      },
      "source": [
        "As we see the second and the fourth sententes were padded to the length of the first and the third senteces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbZR9TdvlGwE"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM-PNvlPrXTo"
      },
      "source": [
        "We also should define metrics we want to evaluate to check quality of model. We will need to define a `compute_metrics()` function that takes the arrays of predictions and labels, and returns a dictionary with the metric names and values.\n",
        "\n",
        "The traditional framework used to evaluate token classification prediction is [seqeval](https://github.com/chakki-works/seqeval). To use this metric, we first need to install the seqeval library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDLw4ywMlIpA"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate seqeval -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi8dbM83sPYt"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"seqeval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlMfc23gstO_"
      },
      "source": [
        "This `compute_metrics()` function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we donâ€™t need to apply the softmax). Then we have to convert both labels and predictions from integers to strings (`seqeval` works with strings, not with indexes). We remove all the values where the label is $-100$, then pass the results to the `metric.compute()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4ToYRj7sRMR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[idx2tag[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [idx2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "        # all_metrics also contains metrics inside each of the labels, you can track them too\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeeQG8f2YgyV"
      },
      "source": [
        "## BERT (0.5 - 3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qet1SJtBYlNE"
      },
      "source": [
        "The sequence labeling task is a degenerate case of the seq2seq task: we need to map a sequence of words to a sequence of labels (tags) of **the same length**.\n",
        "\n",
        "In case of BERT we want to get a vector of probabilities of labels for each input token. The simplest way to make it is just feed output token embeddings to Linear layer.\n",
        "\n",
        "`transformers` class `BertForTokenClassification` works just like this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poL6fQ1wYpCZ"
      },
      "source": [
        "![image.png](https://d2l.ai/_images/bert-tagging.svg)\n",
        "\n",
        "Image from FastAI article: https://d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgsw0wE8YiVV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,  # we declared it earlier, with tokenizer\n",
        "    id2label=idx2tag,\n",
        "    label2id=tag2idx,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z486ZMrilxiF"
      },
      "source": [
        "The last layer `classifier` was changed as we have different classes in our task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxcmLOGrlaMA"
      },
      "outputs": [],
      "source": [
        "model.config.num_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1eg_NB6miZW"
      },
      "source": [
        "As we would like to use HuggingFace training API now we should specify training argumets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F8mtGWYmWq3"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "experiment_name = \"rubert-finetuned-ner\"\n",
        "args = TrainingArguments(\n",
        "    output_dir=experiment_name,\n",
        "    evaluation_strategy=\"epoch\",  # evaluate metrics every epoch\n",
        "    save_strategy=\"epoch\",  # save model checkpoints every epoch\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        "    logging_steps=100,\n",
        "    run_name=experiment_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5yL8Z1tm-Wk"
      },
      "source": [
        "Once this done, let's begin the process (relax and drink a cup of tea)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZFBE2Ivm5_P"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaIsJgfYt8g1"
      },
      "source": [
        "It is playing time! Try your best to achive highest metrics!\n",
        "\n",
        "It is graded task, supass `Overall F1`:\n",
        "- $0.88$ to get $0.5$ point\n",
        "- $0.90$ to get $1.5$ points\n",
        "- best result will be awarded with $3$ points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SshKkxD0VKR"
      },
      "source": [
        "## Custom LoRA (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps6fIjCj1LBN"
      },
      "source": [
        "Training some DL models is time-, GPU- and data- consuming, e.g. imagine fine-tuning LLM's... Instead, you can use low-rank adapters based on [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n",
        "\n",
        "The core idea is to add low-rank adapters __in parallel with attention projection matrices,__ like this:\n",
        "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ngr-KccyuDuX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Apply self.module and LoRA adapter, return the sum (base module outputs + adapter outputs)\n",
        "        # YOUR CODE HERE\n",
        "        return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyMngbVw1wH6"
      },
      "outputs": [],
      "source": [
        "# test your implementation\n",
        "test_linear = nn.Linear(128, 128)\n",
        "test_linear.weight.data[...] = torch.eye(128)\n",
        "test_adapter = LoRALayer(test_linear, rank=8)\n",
        "\n",
        "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
        "\n",
        "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
        "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
        "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
        "\n",
        "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128), torch.linspace(-1, 1, 128).unsqueeze(0))\n",
        "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
        "dummy_loss.backward()\n",
        "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
        "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
        "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
        "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
        "del dummy_loss, test_linear, test_adapter\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An2Mwlls1-KJ"
      },
      "source": [
        "Now let's apply it to our BERT!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZeyexcA39DH"
      },
      "outputs": [],
      "source": [
        "custom_lora_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=idx2tag,\n",
        "    label2id=tag2idx,\n",
        ")\n",
        "\n",
        "# freeze pretrained weights\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anXECwUJ157V"
      },
      "outputs": [],
      "source": [
        "for name, module in custom_lora_model.named_modules():\n",
        "    if 'BertSdpaSelfAttention' in repr(type(module)):\n",
        "        module.query = LoRALayer(module.query, rank=8)\n",
        "        module.key = LoRALayer(module.key, rank=8)\n",
        "        module.value = LoRALayer(module.value, rank=8)\n",
        "\n",
        "assert sum(isinstance(module, LoRALayer) for module in custom_lora_model.modules()) == 36  # for DeepPavlov/rubert-base-cased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O50qDm34SIr"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"rubert-custom-lora-ner\"\n",
        "args = TrainingArguments(\n",
        "    output_dir=experiment_name,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        "    logging_steps=100,\n",
        "    run_name=experiment_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3NgfDV84Z1m"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=custom_lora_model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to achieve at least `Overall F1` $= 0.8$"
      ],
      "metadata": {
        "id": "wGPV03uwAqdx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esS69JBC5HmW"
      },
      "source": [
        "As we made LoRA layers only for attention projections, it may perform worse than fully finetuned model, but you can try to make LoRA for all linear layers. Also rank of LoRA plays crucial role while using small values.\n",
        "\n",
        "Also you can try bigger models to achieve higher scores, as with LoRA it costs you less GPU GB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qPGrXit9nGF"
      },
      "source": [
        "## PEFT LoRA (0.5 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cHZtdVu5XbG"
      },
      "source": [
        "In real life you don't want to create `LoRALayer` each time, so you will use libararies. One of the most popular is [`PEFT`](https://github.com/huggingface/peft) (Parameter Efficient Fine-Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFuJMxK65MIR"
      },
      "outputs": [],
      "source": [
        "!pip install peft -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY6oPKpo_AsI"
      },
      "source": [
        "Now we just need to wrap our model and set LoRA arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfLYhWJv8PVf"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.TOKEN_CLS,\n",
        "    inference_mode=False,\n",
        "    r=8,  # rank of LoRA\n",
        "    lora_alpha=32,  # scaling factor\n",
        "    lora_dropout=0.1,  # dropout is commonly used in many LoRA implementations\n",
        ")\n",
        "\n",
        "peft_lora_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=idx2tag,\n",
        "    label2id=tag2idx,\n",
        ")\n",
        "peft_lora_model = get_peft_model(model, peft_config)\n",
        "peft_lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP_yFCeDi5UZ"
      },
      "source": [
        "Only 0.18% of params is trainable -- much easier to optimize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkuGRSZ1_KrS"
      },
      "source": [
        "That's all! Now try to train it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHNHPsjn9u1Z"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"rubert-peft-lora-ner\"\n",
        "args = TrainingArguments(\n",
        "    output_dir=experiment_name,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        "    logging_steps=100,\n",
        "    run_name=experiment_name,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_lora_model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP82xbt_Ac2d"
      },
      "source": [
        "## Visualize results (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYmTWlq8bmr-"
      },
      "source": [
        "Compare predictions of your models with groud truth, to pretty visualizing try `ner_render()`. What type of mistakes do your models make?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "c1YVAtNKE19M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}